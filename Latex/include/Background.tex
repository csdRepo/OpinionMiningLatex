% CREATED BY DAVID FRISK, 2015
\chapter{Background}\label{ch_2}
In order to understand, and create the individual parts that were mentioned in Chapter 1 and to interconnect them in total, studying and understanding of both basic and state of the art technologies in the field of Computer Science were essential. Some of the technologies that we studied and used to write this Thesis were the following:\\
\begin{itemize}

	\item Machine Learning (\ref{21_ref})
	\item POS-Tagging (\ref{22_ref})
	\item Information Retrieval (\ref{23_ref})\\

\end{itemize}
Also, a deep understanding of Greek Grammar and of the ways an argument is expressed were necessary.

\section{Machine Learning}\label{21_ref}
Machine Learning is a field of Computer Science that deals with the study and construction of Algorithms that can make predictions about the data that were set as entry. These algorithms function with the construction of a model based on data that exist as entry examples, for prediction and decision making. They are based on these data instead of following strict static instructions. This technology has a wide use in problem solving where a pre-designed algorithm is impossible to use. Sometimes, (like in the case of this Thesis), Machine Learning is confused with Data Mining even though its main focus is the investigative analysis of data. Examples of Machine Learning application are ``Spam Filtering'', ``Optical Character Recognition (OCR)'', ``Search Engines'', ``POS-Tagging'' etc.

\subsection{Categorization of Machine Learning Algorithms}\label{211_ref}
The classifications that can be made based on the desired output of the Machine Learning System are the following:\\
\begin{itemize}

	\item \textbf{Classification:} In this category, the inputs are divided in two or more classes and the system must produce a model that classifies unknown entries in one of these classes. ``Spam filtering'' is an example of such classification.
	\item \textbf{Regression:} In this category, the outputs are continuous and not distinguishable. This is usually handled in a monitored way.
	\item \textbf{Clustering:} In this category, a total number of inputs must be divided in groups. In contrast with classification, the groups are not known beforehand. For this reason, it can be characterised as an uncontrolled process.
	\item \textbf{Destiny Estimation:} This category finds the allocation of inputs in a place.
	\item \textbf{Dimension Ability Reduction:} This category simplifies the inputs by mapping them out in a lower-dimensional place. An example is when a list of documents is given in natural language and recognising which of these files cover similar topics is needed.\\

\end{itemize}

\subsection{Machine Learning Algorithms}\label{212_ref}
Examples of known algorithms which were used in this Thesis follow. In chapter four, the way they were used will be thoroughly analysed.\\
\begin{itemize}

	\item \textbf{Naive Bayes\footnote{\url{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}}:} Naive Bayes is an algorithm of probabilistic classification which is based on Bayes Theorem (or Bayes rule), with strongly independent admissions. It is classified in the ``Classification'' category. The simple admission of the Algorithm is:\\

\begin{equation}
P\left(C_{k}|x\right)=\frac{P\left(C_{k}\right)P\left(x|C_{k}\right)}{P\left(x\right)}
\end{equation}
\\
\begin{example}
	``To predict the possibility of rain we usually use some indications like the density of dark clouds in the sky.''\\
\end{example}

\begin{equation}
P\left(raining|darkcloud\right)=\frac{P\left(raining\right)P\left(darkcloud|raining\right)}{P\left(darkcloud\right)}
\end{equation}
\\
\begin{itemize}

	\item P(darkcloud/raining) is the possibility of dark clouds when it rains.
	\item P(raining) is the possibility of rain in advance.
	\item P(darkcloud) is the possibility of a dark cloud in the sky.\\

\end{itemize}


	\item \textbf{Support Vector Machines(SVM)\footnote{\url{https://en.wikipedia.org/wiki/Support_vector_machine}}:} SVM algorithms monitor learning models connected to learning algorithms which analyse data and recognise templates. They are used for classification and regression analysis. They work taking into account a total of education models which belong to one of the two categories. An SVM education algorithm constructs a model which allocates new examples to one or the other category, making it non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped out in such a way that the examples of the separate categories are divided by a clear gap which is as broad as possible. The new examples are then matched to that same space and are predicted to belong to one category based on which side of the gap they fall on. The model of the linear algorithm  SVM is:\\
\\
\textit{Given a train set D, a total of n points of the form:}
\\
\begin{equation}
D=\left[(x_{i},y_{i})|x_{i}\in R^{p},y_{i}\in{-1,1}\right]_{i=1}^{n}
\end{equation}
\\
where $y_{i}$ is either 1 or -1, which indicates the category to which spot $x_{i}$ belongs. Each $x_{i}$ is a $P$-Dimensions ``real vector''. We want to find the hyperplane of maximum margin which separates the spots which have $y_{i}=1$ from those that have $y_{i}=-1$.

\end{itemize}
\section{Part-Of-Speech Tagging (POS-Tagging)}\label{22_ref}
In the field of Linguistics, POS-Tagging\footnote{\url{https://en.wikipedia.org/wiki/Part-of-speech_tagging}}, which is the process of tagging a word in a corpus which matches a specific part of speech, is based  on its definition and on its frame, i.e. its relation to other close and relevant words in a phrase, sentence or paragraph. A simplified form is the characterisation of the words as nouns, verbs, adjectives, adverbs etc. POS-Tagging is more difficult than having a list of words and the part of speech each one is, because some words can represent different parts of speech for different periods of time. This is not rare in natural language (in contrast with many artificial languages), where many words have an ambiguous meaning and use. For this reason, the application of Machine Learning is necessary. In more advanced POS-Tagging tools, techniques of Machine Learning are applied, like the Support Vector Machine algorithm that we described in subsection (\ref{21_ref}).\\
\\
In this Thesis, a ready-made tool for POS-Tagging, ``ILSP POS-Tagger'' was used, which is based on a dictionary but also uses machine learning algorithms. This tool offers very good precision. More details about it will be given in Chapter 4.

\section{Information Retrieval}\label{23_ref}
Information retrieval(IR)\footnote{\url{https://en.wikipedia.org/wiki/Information_retrieval}} is the scientific field of Computer Science that studies effective ways to search for information and data in files and meta-data relevant to files, as well querying data bases and the World Wide Web. Information Retrieval is based on the data base theory, on suitable informative systems and on mathematical methods of Artificial Intelligence like the ones mentioned in Chapter (\ref{22_ref}), broad use takes place in the World Wide Web and especially by specialists in all search engines (e.g. Google). Someone can say that it has an interdisciplinary character as it borrows elements from Computer Science, Mathematics, Library Science, Cognitive Psychology, Linguistics and Statistics.\\
\\
A process of information retrieval begins when the user enters a query in the system. The queries are research requests in some data base, like for example the search of an alphanumeric string in some search engine of the Web. In Information Retrieval a query does not precise uni-vocally an item, but may times many items can match the query, maybe in different degrees of relevance.\\
\\
An item is an entity that is represented by some information in a data base. In order to answer the user's queries, research among the data of the base for possible answers is done. The data can be text documents, pictures, sounds or video archives.\\
\\
Most information retrieval systems calculate a numerical score about how well each item in the data base matches the query and classify the items according to this calculation. The items that are on top of the list appear in front of the user. The process can be repeated afterwards, if the user wishes to improve the query.

\subsection{Information Retrieval Algorithms}\label{231_ref}
Example of a well-known algorithm follow, which were also used in this Thesis. In Chapter 4, the way they were used will be analysed in detail.\\
\begin{itemize}

	\item \textbf{TF-IDF (Term Frequency-Inverse Document Frequency):} The TF-IDF algorithm is an arithmetic statistical element whose goal is to reflect how important a word in a file or in a collection, is. Often, it is used as a weighting factor in information and text retrieval. The value of TF-IDF increases in proportion to the number of times that a word appears in a text, but is counterbalanced by the frequency of the word that exists in the collection, which helps to adapt to the fact that some words appear, in general, more frequently.\\
\\
To interpret TF-IDF it is necessary to note that it is a product of two algorithms (term frequency, inverse document term frequency) which must be explained.\\
\\
The term frequency $tf_{t'd}$ of the term t in a file is defined as the number of times that $t$ appears in $d$.
\\
\begin{equation}
tf\left(t,d\right)=0.5+\frac{0.5*f\left(t,d\right)}{max\left\lbrace\left(t,d\right):t \in d\right\rbrace}
\end{equation}
\\
Inverse document frequency is a way to measure how much information each word contains, i.e. if the term is frequent or rare in all the files. It is the logarithmically weighted fraction of the files that contain the word. It comes up from the division of the total number of files from the number of files that contain the term. In that way, we define the IDF (Inverse Document Frequency) of t as:
\\
\begin{equation}
idf\left(t,D\right)=log*\frac{N}{|\left\lbrace d \in D :t \in d\right\rbrace|}
\end{equation}
\\
Where:\\
\begin{itemize}

	\item $N$ is the total number of the collection's texts.
	\item $|\left\lbrace d \in D :t \in d\right\rbrace|$ the number of the texts that the term appears in.So,as TF-IDF is defined the product of $tf$ and $idf$.

\end{itemize}

\end{itemize}
\section{Related Work in Argument extraction}\label{24_ref}
One of the first approaches for argument identification is presented in the Doctoral Thesis  entitled ``Argumentative Zoning'', written by Teufel (1999). After this Thesis, there have been a variety of projects and Theses that are focused on how to use the public's opinions that are available in texts of electronic form, aiming at defining social tendencies and analysing their political stance. For example, the WeGov toolbox[1] which is an online tool (http://wegov-project.eu) that gives someone the possibility to search for topics and views from different social networks. It supports the analysis and the brief presentation of views and discussions and ultimately helps those in charge of Politics to publish the information that has been extracted from the Social Media. The system even predicts stances that are expected to attract more attention.\\
\\
An application focused on Sentiment Analysis in political comments on Twitter is the Thesis of Diakopoulos and Ahamma[2] Diakopoulos and Shamma,2010) which associate timelessly the dynamics of a sentence in a Twitter discussion. For this goal, the writers use a commentary process based on Mechanical Turk (which is supported by many filters) to note the Tweets that are relevant to the discussion. In the Thesis, the total sentiment of Tweets was studied. Another study from Tumasjan et al. [3] verifies that micro blogging (Twitter) is widely used for political discussion and expression of political view. In this Thesis,to achieve Sentiment Analysis, the writers used LIWC2007 which is a tool for Language Analysis.\\
\\
One more Thesis which attempts to extract arguments and separate them in Claim and Justification in texts in Greek found in Social Media is ``Argument Extraction from News,Blogs and Social Media''[4] and achieves a total precision by 72\% and recall by 79\%.\\
\\
Finally, this Thesis was based a lot on a paper of [7] Christian Stab and Iryna Gurevych which is divided in two basic steps. First of all, in tracking the elements of the argument which are either ``justification'' or ``claim'' and secondly to define pairs from these components which together form a complete Argument. In their experiment, the results were  F1-Score 72.6\% for tracking the argument's components and 72,2\% for tracking the connections that constitute a complete argument.
