\definecolor{gray}{rgb}{0.4,0.4,0.4}
\definecolor{darkblue}{rgb}{0.0,0.0,0.6}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  showstringspaces=flase,
  commentstyle=\color{gray}\upshape
}

\lstdefinelanguage{XML}
{
  morestring=[b]",
  morestring=[s]{>}{<},
  morecomment=[s]{<?}{?>},
  stringstyle=\color{black},
  identifierstyle=\color{darkblue},
  keywordstyle=\color{cyan},
  morekeywords={xmlns,version,type}% list your attributes here
}


% CREATED BY DAVID FRISK, 2015
\chapter{Methods}
In this chapter, we will thoroughly analyse the ways with which the three processing stages which were presented in the previous units, were implemented.\\
\\
In order to describe in the best possible way the process that was followed, a detailed description of the dataset which was used will be given, as well as of the features that characterise it. Then, we will describe the relational database model which we used to store the information from the texts (dataset). Finally, for each one of the process stages, we will describe the methodology with which each matter was approached.

\section{Preparing the Dataset}\label{31_ref}
In this part of the methodology, the features of the used dataset (\ref{311_ref}) will be described in detail. Some information on the way of choosing data (\ref{312_ref}) will be described as well. Next, the way of data mining from the Greek Open Government platform\footnote{\url{http://www.opengov.gr}} (\ref{313_ref}) and finally, the Entity-Relation Model (\ref{314_ref}) of the database which was used to store the data will also be described.

\subsection{Dataset} \label{311_ref}
As it has already been mentioned in some points of this Thesis, the data that were used have been taken from the Greek Open Government platform, which constitutes a platform of electronic consultation of citizens on texts, more specifically on laws and decrees that the Greek Government issues. These data are open and accessible to everyone.\\
\\
In this section, the basic features of the studied texts will be described. The reason why this section comes first in this part of the methodology, is that the very nature of these texts (they are basically users' comments to the online service), created many problems in their analysis.\\
\\
As it has already been mentioned, the texts that were studied feature several oddities, some of which made the process of analysing them difficult.\\

\begin{itemize}
  \item Initially, the first that we can notice is that the length of the texts is 	relatively short. To be precise, it is rare for them to be longer than 3000 characters (approximately 200 words, 80\% of the texts). The length of the text did not affect all the stages of the analysis. The biggest difficulty appeared in the effort to extract the degree of the writer's agreement with the initial text (more details will be given later).\\
  
  \item A second remark is the fact that the texts that were studied do not consist an official text. By the term ``official'', we want first to declare that the texts are made up of users' comments in an online service and secondly, that they contain many errors (spelling etc). This created many difficulties in the studying of these comments. The first difficulty had to do with the tools needed in order to conduct the overall analysis of each text. The basic idea was that the tools had to be tolerant when it came to errors, at least up to a degree.\\
\\
Some very usual errors are:

  	\begin{enumerate}
  	\item spelling errors
  	\item absence of some letters in a word
  	\item letter transposition in a word
  	\item use only of capital letters
  	\item absence of punctuation
  	\item wrong sentence separation (there was no gap after the dot)
  	\item some more errors that will not be mentioned for ease of reference\\
	\end{enumerate}

  \item One more issue is  that there are many times when syntactic structure errors are 	spotted. This problem is directly connected to the use of POS-Tagger for the syntactic analysis (parsing) of texts. This issue affects, to some degree, the extrapolation of arguments and of proposals and counter proposals that the user makes.\\
  
  \item Another feature is that the texts are entirely in Greek. This problem is more 	serious, because there are no tools which we needed at some point of the analysis, that support the Greek Language. Subsequently, as we will see later on, there was the need to resort to some compromising solutions.\\
  
  \item One last issue that is worth mentioning, which constitutes a more qualitative feature, at least in the whole of texts that were studied, is the fact that the majority of users who wrote a comment are ``annoyed''. This ``annoyance'' stems from the fact that the texts that are under discussion contain laws and presidential decrees that, essentially, lead to a decrease in public spending towards the citizens. This ``annoyance'' is noted almost in the entire dataset that we studied. The problem is that the texts in which the writers agree with the initial text are limited. As a result, this issue complicates the process of acknowledging, if the writer agrees with the initial text.
\end{itemize}


\subsection{Choosing Set of Documents}\label{312_ref}
To continue the process, we randomly chose five different bills that contained a significant amount of users' replies. Afterwards, we selected a few, trying to eliminate the replies that we did not want to process. For example:\\

\begin{itemize}

	\item replies that only contained one sentence
	\item replies in greeklish\\

\end{itemize}

Next, we limited the dataset so as to contain a number of approximately two hundred replies. This total is the final dataset that was studied and on which the conclusions for all the processing stages were based.

\subsection{Finalizing the Dataset}\label{313_ref}
The last step for the creation of the final dataset was the data mining from the Greek Open Government platform\footnote{\url{http://www.opengov.gr}} (the website for public consultation on laws). This process was  simple enough, since the service provides the users with the option to locally store all the comments that have been posted for each law or decree. The data were in excel file format, providing for each comment the following meta-data:\\

\begin{itemize}

	\item the Law Article which was commented
	\item an id for each comment
	\item the name of the user-commenter
	\item the date\\

\end{itemize}

These data were later stored in the database which was created for the storage of data that were collected in all the stages of processing.

\subsection{Database}\label{314_ref}
In order to store the database, an MySQL\footnote{\url{https://www.mysql.com}} Database was created, whose Entity-Relation Model\footnote{\url{https://en.wikipedia.org/wiki/Entity\%E2\%80\%93relationship_model}} can be presented in the following layout.

\begin{figure}[H]
\centering
\includegraphics[width=1.15\linewidth]{figure/ER.pdf}
\caption{Entity - Relation Model}
\end{figure}

In the above layout, we can see the Entity-Relation Model that was used for storing data. The keys for each table of data have been marked with bold.

\subsection{Building a Trainset}\label{315_ref}
One last element that deals with the chapter on dataset, is characterising a total of sentences if each one of them contains an Argument or a Suggestion. We should note here that sentence separation will be analysed thoroughly later. The Train set that we created, as we will see in the chapters that follow, is needed so that the machine learning algorithms can become train, as well as to achieve a better evaluation. The set of sentences that was created contains approximately one thousand sentences.


\section{Argument Extraction}\label{32_ref}
At this point, the process with which the Argument Extraction was carried out on the whole of the texts will be described. The process is based on three stages:\\
\\
\begin{itemize}

	\item Selecting Argument Markers (\ref{321_ref}).
	\item POS-Tagging the set of documents (\ref{322_ref}).
	\item Applying machine learning  to the Argument Markers (\ref{323_ref}).\\

\end{itemize}
Each one of the three stages will be explained right away.

\subsection{Selecting Argument Markers}\label{321_ref}
This step, in essence, constitutes the selection process of certain ``criteria'' that will help us define what an argument is. Essentially, this process constituted the hardest part in the whole Argument Extraction stage. This step, by extension, was performed only once.\\
\\
Having studied enough from Related Work (which was mentioned to a great extent above), it was clear that in order to extract the arguments from the texts that we had available we would have to apply some Machine Learning process. Therefore, it is now clearer that at this step (\ref{321_ref}), we had to define some parameters which will define with a relative clarity whether a sentence is an argument or not.\\
\\
But before we get in the process of searching for these variables, we had one more difficulty to face. Even though it sounds relatively easy to recognise the arguments in a text, in reality it was a rather difficult process. There was great difficulty for the whole team that is working on this Project to agree on whether a sentence contains an argument or not. After a lot of discussion, we ended up with the following definition:\\
\\
\textit{Sentence is likely to contain an argument if it contains the following markers:\\}
\begin{itemize}

	\item \textit{The sentence provides a context clue from which we can interpret that the writer expresses an opinion}.
	\item \textit{The sentence is explanatory, which means that the writer wants either to further explain or support an opinion.\\}
	
\end{itemize}

If there is any of the above markers in a sentence, then this sentence can be characterised as an argument (Argumentative sentence). Even with the above two conventions, in some cases it is still difficult to determine whether a sentence is an argument. More precisely, another convention has been set, with which we made an effort to exclude the sentences that are interrogative. The reason is that usually, interrogative sentences are likely to express irony. In our dataset this was actually very usual.\\
\\
Having set the above conventions, we studied a total of Argumentative Sentences, in order to be able to track down ``Argument Markers''. In essence, we looked for parts of speech (for example the number of verbs, number of adjectives and more), as well as other variables that often appear in this type of sentences. These variables were to be used in the next steps and especially in the step where the ``Machine Learning'' process is applied\\
\\
From the study that was conducted, also combining Related work, we ended up with the following variables:\\
\\
\begin{itemize}

	\item \textbf{Number of Verbs:} In many studies which have been conducted in the past, it was noted that verbs are closely linked to arguments. To be precise, verbs express action. Another characteristic is that verbs syntactically compose a sentence or, a little more arbitrarily, verbs enrich a sentence. Consequently, it was noted that the sentences which contain arguments usually have a larger number of verbs.
	\item \textbf{missing:}	
	\item \textbf{Key Words:} This variable refers to the number of words that were traced in a group of words that we created. This group contains words that are used when someone tries to explain or support an opinion. For example, in this group there are the words  consider, believe, admit, suppose, think, must, consequently, since, until, because, namely.
	\item \textbf{Number of Connective Words:} This variable counts the number of linking words that were found in a sentence. In essence, this number has a direct relation to the next variable which shows the total number of words. The idea of counting the length of a sentence lies to the fact that usually, longer sentences are more likely to contain arguments. Especially when the studied dataset contains texts with arguments and political discussion.
	\item \textbf{Total Number of Words:} respectively with the previous variable.
	\item \textbf{Average Number of Letters in a Word:} This variable came up from the bibliography that we had available. It has been noted that this variable, especially in texts that contain political discussion, can help in a significant way to track arguments.
	\item \textbf{Number of Adjectives:} It is the number of arguments that were found in a sentence. The logic behind this variable is the same  with  the ``Number of Verbs'' variable logic.
	\item \textbf{Number of Adverbs:} It came up after studying the bibliography and it states the number of adverbs in a sentence.
	\item \textbf{Number of Nouns:} The role of this variable is equivalent to the ``Number of Adjectives'' variable.
	\item \textbf{A Boolean Variable that States Whether a Sentence is Interrogative:} The need for this variable was explained thoroughly above.

\end{itemize}

\subsection{POS-Tagging}\label{322_ref}
In this part we will analyse the second step in the process of argument extraction from a text. Unlike the previous step (\ref{321_ref}), the execution of this step is essential for every new step we wish to analyse.\\
\\
As it has already been mentioned above (\ref{22_ref}), the POS-Tagging procedure has as goal to analyse the grammar of the text, as well as to extract an output which will contain a recognition of what part of speech each word is. Clearly, this process is very important for Argument Extraction because it constitutes the way with which all parts of speech will be detected.

\subsubsection{POS-Tagger}\label{3221_ref}
In this Thesis, ``ILSP POS-Tagger\footnote{\url{http://nlp.ilsp.gr/soaplab2-axis/}}'' was used, which was created by the ``Institute of Language and Speech processing\footnote{\url{http://www.ilsp.gr/en}}''.

\subsubsection{POS-Tagger Output}\label{3222_ref}
From the outputs supported by POS-Tagger that we had available, we have chosen the ``xceslemma'' option, with which-apart from POS-Tagging, Lemmatization can also be accomplished. We will need the latter in the next unit that we are going to study. The output given is in XML format. An example can be seen below:\\
\\
\lstset{language=XML,
  morekeywords={id,word,lemma,tag}
}
\begin{lstlisting}[frame=single, basicstyle=\small]
<?xml version='1.0' encoding='UTF-8'?>
<cesDoc xmlns="http://www.xces.org/schema/2003" version="0.4">
  <text>
    <body>
      <p id="p1">
        <s id="s1">
          <t id="t1" word="..." tag="AtDfNeSgNm" lemma="..."/>
          <t id="t2" word="..." tag="RgFwOr" lemma="..."/>
          <t id="t3" word="..." tag="PnReNe03SgNmXx" lemma="..."/>
          <t id="t4" word="..." tag="VbMnIdPr03SgXxIpPvXx" lemma="..."/>
          <t id="t5" word="..." tag="VbMnIdPr03SgXxIpPvXx" lemma="..."/>
          <t id="t6" word="..." tag="AsPpSp" lemma="..."/>
          <t id="t7" word="..." tag="NoCmFeSgAc" lemma="..."/>
          <t id="t8" word="..." tag="RgFwOr" lemma="..."/>
          <t id="t9" word="..." tag="PTERM_P" lemma="..."/>
        </s>
      </p>
    </body>
  </text>
</cesDoc>
\end{lstlisting}

The text that was given as input was \textit{``The output which is given is in XML format''}. We note that for every word of the text, the following information is given:\\
\\
\begin{itemize}
	
	\item \textbf{Id:} an auto increment identifier given in every word of the text
	\item \textbf{Word:} the initial word of the text
	\item \textbf{Tag:} the Grammar concerning the particular word. For example, verbs start with ``Vb'' and nouns with ``No''. The whole tagset can be found here \url{http://nlp.ilsp.gr/nlp/tagset_examples/tagset_en/}
	\item \textbf{Lemma:} it is the dictionary entry of each word. For example, we note that the verb ``given'' has ``give'' as its lemma.

\end{itemize}

\subsubsection{Parsing XML File}\label{3223_ref}
TODO\\
\\
\subsubsection{Uploading to Database}\label{3224_ref}
TODO\\
\\
\lstset{language=SQL}
\begin{lstlisting}[frame=single, basicstyle=\small]
INSERT INTO opngv_argument VALUES (values..)
\end{lstlisting}

\subsection{Apply Machine Learning}\label{323_ref}
TODO\\
\\
\subsubsection{Selecting Train and Test Set}\label{3231_ref}
TODO\\
\\
\lstset{language=SQL}
\begin{lstlisting}[frame=single, basicstyle=\small]
SELECT
	opngv_argument.verbs,
	opngv_argument.pv_verbs,
	opngv_argument.cue_words,
	opngv_argument.connective_words,
	opngv_argument.total_words,
	opngv_argument.word_mean_length,
	opngv_argument.adjective,
	opngv_argument.adverbs,
	opngv_argument.noons,
	opngv_argument.question,
	opngv_trainset.Argument
FROM
	opngv_sentence
	INNER JOIN opngv_argument
		ON opngv_sentence.comment_id = opngv_argument.comment_id
	 	AND opngv_sentence.sentence_id = opngv_argument.sentence_id
	INNER JOIN opngv_trainset
	 	ON opngv_sentence.comment_id = opngv_trainset.comment_id
	 	AND opngv_sentence.sentence_id = opngv_trainset.sentence_id

\end{lstlisting}
TODO\\
\\

\subsubsection{Machine Learning Process}\label{3232_ref}
TODO\\
\\
\subsubsection{Machine Learning Algorithms}\label{3233_ref}
TODO\\
\\

\textit{“Weka is a collection of machine learning algorithms for data mining tasks. The algorithms can either be applied directly to a dataset or called from your own Java code. Weka contains tools for data pre-processing, classification, regression, clustering, association rules, and visualization. It is also well-suited for developing new machine learning schemes.”}

\section{Suggestion Extraction}\label{33_ref}
TODO\\
\\
\subsection{Selecting Suggestion Markers}\label{331_ref}
TODO\\
\\

\lstset{language=XML,
  morekeywords={id,word,lemma,tag}
}
\begin{lstlisting}[frame=single, basicstyle=\small]
<?xml version='1.0' encoding='UTF-8'?>
<cesDoc xmlns="http://www.xces.org/schema/2003" version="0.4">
  <text>
    <body>
      <p id="p1">
        <s id="s1" casing="lowercase">
          <t id="t1" word="..." tag="VbIsIdPr03SgXxIpAvXx" lemma="..."/>
          <t id="t2" word="..." tag="PtSj" lemma="..."/>
          <t id="t3" word="..." tag="VbMnIdXx03SgXxPePvXx" lemma="..."/>
          <t id="t4" word="..." tag="NoCmFeSgNm" lemma="..."/>
          <t id="t5" word="..." tag="AsPpPaFeSgAc" lemma="..."/>
          <t id="t6" word="..." tag="NoCmFeSgAc" lemma="..."/>
          <t id="t7" word="..." tag="DIG" lemma="..."/>
          <t id="t8" word="..." tag="AtDfMaSgGe" lemma="..."/>
          <t id="t9" word="..." tag="NoCmMaSgGe" lemma="..."/>
          <t id="t10" word="..." tag="PTERM_P" lemma="..."/>
        </s>
      </p>
    </body>
  </text>
</cesDoc>
\end{lstlisting}

TODO\\
\\

\subsection{POS-Tagging and Lemmatization the set of Documents}\label{332_ref}
TODO\\
\\
\subsection{Apply Information Retrieval Methods in order to find the Suggestions}\label{333_ref}
TODO\\
\\
\subsection{Adding additional features for the optimization of Machine Learning Processs}\label{334_ref}
TODO\\
\\
\subsection{Apply Machine Learning}\label{335_ref}
TODO\\
\\
\subsubsection{Selecting Train and Test Set}\label{3351_ref}
TODO\\
\\
\lstset{language=SQL}
\begin{lstlisting}[frame=single, basicstyle=\small]
SELECT
	opngv_suggestion.weight,
	opngv_suggestion.category,
	opngv_suggestion.total_words,
	opngv_trainset.Suggestion
FROM
	opngv_sentence
	INNER JOIN opngv_suggestion
		ON opngv_sentence.comment_id = opngv_suggestion.comment_id
		AND opngv_sentence.sentence_id = opngv_suggestion.sentence_id
	INNER JOIN opngv_trainset
		ON opngv_sentence.comment_id = opngv_trainset.comment_id
		AND opngv_sentence.sentence_id = opngv_trainset.sentence_id
ORDER BY
	opngv_trainset.Suggestion DESC
LIMIT 366
\end{lstlisting}
TODO\\
\\

\subsubsection{Machine Learning Process}\label{3352_ref}
TODO\\
\\
\subsubsection{Machine Learning Algorithms}\label{3353_ref}
TODO\\
\\
\section{Overall Opinion Extraction}\label{34_ref}
TODO\\
\\
\subsection{Translate Documents to English}\label{341_ref}
TODO\\
\\
\subsection{Perform Sentiment Analysis}\label{342_ref}
TODO\\
\\
\begin{itemize}
	\item
	 \textbf{SentiStrength\footnote{\url{http://sentistrength.wlv.ac.uk}}:}
	 \textit{“SentiStrength estimates the strength of positive and negative sentiment in short texts, even for informal language. It has human-level accuracy for short social web texts in English, except political texts. SentiStrength reports two sentiment strengths:}\\

	\begin{itemize}
		\item \textit{-1 (not negative) to -5 (extremely negative)}
		\item \textit{1 (not positive) to 5 (extremely positive)}\\
	\end{itemize}


\textit{Why does it use two scores? Because research from psychology has revealed that we process positive and negative sentiment in parallel - hence mixed emotions.
SentiStrength can also report binary (positive/negative), trinary (positive/negative/neutral) and single scale (-4 to +4) results.”}\\

	\item
	\textbf{Sentiment Analysis with Python NLTK Text Classification\footnote{\url{http://text-processing.com/docs/sentiment.html}}:} 
	\textit{“Sentiment analysis using a NLTK 2.0.4 powered text classification process. It can tell you whether it thinks the text you enter below expresses positive sentiment, negative sentiment, or if it's neutral. Using hierarchical classification, neutrality is determined first, and sentiment polarity is determined second, but only if the text is not neutral.”}
\end{itemize}









